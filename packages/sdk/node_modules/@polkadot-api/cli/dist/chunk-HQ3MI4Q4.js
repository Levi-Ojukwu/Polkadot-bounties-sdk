// src/metadata.ts
import { createClient } from "@polkadot-api/substrate-client";
import * as fs from "fs/promises";
import {
  metadata,
  unifyMetadata,
  v15
} from "@polkadot-api/substrate-bindings";
import { getWsProvider } from "@polkadot-api/ws-provider/node";
import { Worker } from "worker_threads";
import { getObservableClient } from "@polkadot-api/observable-client";
import {
  catchError,
  combineLatest,
  concatMap,
  filter,
  firstValueFrom,
  from,
  map,
  merge,
  mergeMap,
  Observable,
  of,
  switchMap,
  take,
  timer
} from "rxjs";
import { dirname } from "path";
import { fileURLToPath } from "url";
import * as knownChains from "@polkadot-api/known-chains";
import { withPolkadotSdkCompat } from "@polkadot-api/polkadot-sdk-compat";
import { startFromWorker } from "@polkadot-api/smoldot/from-node-worker";
import { getSmProvider } from "@polkadot-api/sm-provider";
import { withLegacy } from "@polkadot-api/legacy-provider";
var workerPath = fileURLToPath(
  import.meta.resolve("@polkadot-api/smoldot/node-worker")
);
var smoldotWorker;
var workerRefCount = 0;
async function getSmoldotWorker() {
  if (!smoldotWorker) {
    const worker = new Worker(workerPath, {
      stdout: true,
      stderr: true
    });
    const client = startFromWorker(worker);
    smoldotWorker = [client, worker];
  }
  return smoldotWorker;
}
var getMetadataCall = (provider, at) => new Observable((observer) => {
  const substrateClient = createClient(provider);
  const client = getObservableClient(substrateClient);
  const { runtime$, unfollow, genesis$, getRuntime$ } = client.chainHead$();
  const archive = client.archive(getRuntime$);
  const getRuntimeCtx$ = () => {
    if (!at) return runtime$.pipe(filter(Boolean));
    const hash$ = at.length < 32 && /^\d+$/.test(at) ? from(substrateClient.archive.hashByHeight(Number(at))).pipe(
      map(([hash]) => {
        if (!hash) {
          throw new Error(`Can't find block number "${at}"`);
        }
        return hash;
      })
    ) : of(at);
    return hash$.pipe(switchMap(archive.getRuntimeContext$));
  };
  const subscription = combineLatest({
    runtime: getRuntimeCtx$(),
    genesis: genesis$
  }).pipe(
    take(1),
    map(
      ({
        runtime: {
          lookup: { metadata: metadata2 },
          metadataRaw,
          codeHash
        },
        genesis
      }) => ({
        metadata: metadata2,
        metadataRaw,
        codeHash,
        genesis
      })
    )
  ).subscribe(observer);
  subscription.add(() => {
    try {
      unfollow();
      client.destroy();
    } catch {
    }
  });
  return subscription;
});
var getChainSpecs = (chain) => {
  if (!(chain in knownChains)) {
    const relayChainName2 = JSON.parse(chain).relay_chain;
    return {
      potentialRelayChainSpecs: relayChainName2 in knownChains ? [knownChains[relayChainName2]] : [],
      chainSpec: chain
    };
  }
  const relayChainName = Object.keys(knownChains).find(
    (c) => c !== chain && chain.startsWith(c)
  );
  const potentialRelayChainSpecs = relayChainName ? [knownChains[relayChainName]] : [];
  const chainSpec = knownChains[chain];
  return {
    potentialRelayChainSpecs,
    chainSpec
  };
};
var getMetadataFromSmoldot = async (chain) => {
  workerRefCount++;
  try {
    const [smoldot] = await getSmoldotWorker();
    const chainSpecs = getChainSpecs(chain);
    const potentialRelayChains = await Promise.all(
      chainSpecs.potentialRelayChainSpecs.map(
        (chainSpec) => smoldot.addChain({ chainSpec })
      )
    );
    const provider = getSmProvider(
      smoldot.addChain({
        chainSpec: chainSpecs.chainSpec,
        potentialRelayChains
      })
    );
    return await firstValueFrom(getMetadataCall(provider));
  } finally {
    workerRefCount--;
    if (workerRefCount === 0) {
      const [smoldot, worker] = smoldotWorker;
      smoldotWorker = null;
      await smoldot.terminate();
      await worker.terminate();
    }
  }
};
var getMetadataCallWithError = (...input) => getMetadataCall(...input).pipe(
  map((value) => ({ success: true, value })),
  catchError((error) => of({ success: false, error }))
);
var getMetadataFromWsURL = (wsURL, at) => firstValueFrom(
  merge(
    getMetadataCallWithError(withPolkadotSdkCompat(getWsProvider(wsURL)), at),
    timer(3e3).pipe(
      concatMap(
        () => getMetadataCallWithError(
          getWsProvider({
            endpoints: [wsURL],
            innerEnhancer: withLegacy()
          }),
          at
        )
      )
    )
  ).pipe(
    mergeMap((x, idx) => {
      if (x.success) return [x.value];
      if (idx < 1) return [];
      throw x.error;
    })
  )
);
async function getMetadata({
  metadata: metadataFile,
  codeHash,
  genesis,
  ...entry
}) {
  if (metadataFile) {
    const data = await fs.readFile(metadataFile);
    const metadataRaw = new Uint8Array(data);
    let meta;
    try {
      meta = unifyMetadata(metadata.dec(metadataRaw));
    } catch (_) {
      meta = unifyMetadata(v15.dec(metadataRaw));
    }
    return {
      metadata: meta,
      metadataRaw,
      codeHash,
      genesis
    };
  }
  if ("chain" in entry) {
    return getMetadataFromSmoldot(entry.chain);
  }
  if ("chainSpec" in entry) {
    const chainSpec = await fs.readFile(entry.chainSpec, "utf8");
    return getMetadataFromSmoldot(chainSpec);
  }
  if ("wsUrl" in entry) {
    return getMetadataFromWsURL(entry.wsUrl, entry.at);
  }
  return null;
}
async function writeMetadataToDisk(metadataRaw, outFile) {
  await fs.mkdir(dirname(outFile), { recursive: true });
  await fs.writeFile(outFile, metadataRaw);
}

// src/papiConfig.ts
import fsExists from "fs.promises.exists";
import { readPackage } from "read-pkg";
import { mkdir as mkdir2, readFile as readFile2, writeFile as writeFile2 } from "fs/promises";
import { join } from "path";
import { existsSync } from "fs";
var papiFolder = ".papi";
var papiCfgDefaultFile = "polkadot-api.json";
var packageJsonKey = "polkadot-api";
var defaultConfig = {
  version: 0,
  descriptorPath: join(papiFolder, "descriptors"),
  entries: {}
};
async function readPapiConfig(configFile) {
  if (configFile) return readFromFile(configFile);
  const currentVersionLocation = join(papiFolder, papiCfgDefaultFile);
  const currentVersionLocationExists = await fsExists(currentVersionLocation);
  const readConfig = await (currentVersionLocationExists ? readFromFile(currentVersionLocation) : readFromFile(papiCfgDefaultFile)) ?? await readFromPackageJson();
  if (readConfig && !currentVersionLocationExists) {
    await writePapiConfig(void 0, readConfig);
  }
  return readConfig;
}
async function writePapiConfig(configFile, config) {
  if (configFile) return writeToFile(configFile, config);
  if (!existsSync(papiFolder)) {
    await mkdir2(papiFolder);
  }
  return writeToFile(join(papiFolder, papiCfgDefaultFile), config);
}
async function readFromFile(file) {
  const fileExists = await fsExists(file);
  if (!fileExists) return null;
  return migrate(JSON.parse(await readFile2(file, "utf8")));
}
async function readFromPackageJson() {
  const packageJson = await readPackage();
  if (!(packageJsonKey in packageJson)) return null;
  console.warn("Papi config in package.json is deprecated");
  return migrate(packageJson[packageJsonKey]);
}
function migrate(content) {
  if (typeof content.version === "number") {
    return content;
  }
  return {
    ...defaultConfig,
    entries: content
  };
}
async function writeToFile(file, config) {
  if (file === "package.json") {
    throw new Error("Papi config in package.json is deprecated");
  }
  return writeFile2(file, JSON.stringify(config, null, 2) + "\n");
}

// src/commands/generate.ts
import {
  generateInkTypes,
  generateMultipleDescriptors,
  generateSolTypes
} from "@polkadot-api/codegen";
import { getInkLookup } from "@polkadot-api/ink-contracts";
import {
  EntryPointCodec,
  TypedefCodec
} from "@polkadot-api/metadata-compatibility";
import {
  Binary,
  h64,
  Tuple,
  Vector
} from "@polkadot-api/substrate-bindings";
import { spawn } from "child_process";
import { existsSync as existsSync2 } from "fs";
import fsExists2 from "fs.promises.exists";
import fs2, { mkdtemp, rm } from "fs/promises";
import { tmpdir } from "os";
import path, { join as join3, posix, win32 } from "path";
import process2 from "process";
import { readPackage as readPackage2 } from "read-pkg";
import tsc from "tsc-prog";
import tsup from "tsup";
import { updatePackage } from "write-package";

// src/packageManager.ts
import { readdir } from "fs/promises";
import { join as join2 } from "path";
import { execa } from "execa";
var detected = null;
async function detectPackageManager() {
  if (detected) return detected;
  const { packageManager, executable } = await detectByLockFile() ?? detectByEnvironment() ?? getFallback();
  const version = await getVersion(executable);
  return detected = {
    packageManager,
    executable,
    version
  };
}
async function detectByLockFile() {
  try {
    for (let i = 0, dir = "."; i < 5; i++, dir = join2(dir, "..")) {
      const packageManager = await getByLockFile(dir);
      if (packageManager) {
        return {
          packageManager,
          executable: packageManager
        };
      }
    }
  } catch (ex) {
  }
  return null;
}
var lockFileToPackageManager = {
  "pnpm-lock.yaml": "pnpm",
  "yarn.lock": "yarn",
  "bun.lock": "bun",
  "bun.lockb": "bun",
  "package-lock.json": "npm"
};
var lockFiles = new Set(Object.keys(lockFileToPackageManager));
async function getByLockFile(dir) {
  const files = await readdir(dir);
  const lockFile = files.find((v) => lockFiles.has(v));
  return lockFile ? lockFileToPackageManager[lockFile] : null;
}
function detectByEnvironment() {
  const npm_execpath = process.env.npm_execpath;
  if (npm_execpath) {
    const packageManager = Object.values(lockFileToPackageManager).find(
      (manager) => npm_execpath.includes(manager)
    );
    return packageManager ? { packageManager, executable: npm_execpath } : null;
  }
  if (process.env.PNPM_PACKAGE_NAME) {
    return { packageManager: "pnpm", executable: "pnpm" };
  }
  return null;
}
function getFallback() {
  console.warn("Package manager couldn't be detected, fallback to npm");
  return {
    executable: "npm",
    packageManager: "npm"
  };
}
async function getVersion(executable) {
  const res = await execa(executable, ["--version"]);
  return res.stdout;
}

// src/commands/generate.ts
async function generate(opts) {
  if (process2.env.PAPI_SKIP_GENERATE) {
    return;
  }
  const config = await readPapiConfig(opts.config);
  if (!config) {
    throw new Error("Can't find the Polkadot-API configuration");
  }
  const sources = config.entries;
  if (Object.keys(sources).length == 0) {
    console.log("No chains defined in config file");
  }
  console.log(`Reading metadata`);
  const chains = await Promise.all(
    Object.entries(sources).map(async ([key, source]) => ({
      key,
      ...await getMetadata(source),
      knownTypes: {}
    }))
  );
  if (chains.length === 0) {
    console.log("No entries in polkadot-api.json. Nothing to generate.");
    return;
  }
  console.log(`Generating descriptors`);
  await cleanDescriptorsPackage(config.descriptorPath);
  if (!config.options?.noDescriptorsPackage) {
    await addDescriptorsToPackageJson(config.descriptorPath);
  }
  const descriptorsDir = join3(process2.cwd(), config.descriptorPath);
  const clientPath = opts.clientLibrary ?? "polkadot-api";
  const whitelist = opts.whitelist ? await readWhitelist(opts.whitelist) : null;
  const descriptorSrcDir = join3(descriptorsDir, "src");
  const hash = await outputCodegen(
    chains,
    descriptorSrcDir,
    clientPath,
    whitelist
  );
  if (config.ink || config.sol) {
    outputContractCodegen(config, descriptorSrcDir);
  }
  await replacePackageJson(descriptorsDir, hash);
  await compileCodegen(descriptorsDir);
  await fs2.rm(descriptorSrcDir, { recursive: true });
  if (!config.options?.noDescriptorsPackage) {
    await runInstall();
    await flushBundlerCache();
  }
}
async function cleanDescriptorsPackage(path2) {
  const descriptorsDir = join3(process2.cwd(), path2);
  if (!existsSync2(descriptorsDir)) {
    await fs2.mkdir(descriptorsDir, { recursive: true });
    await fs2.writeFile(
      join3(descriptorsDir, ".gitignore"),
      "*\n!.gitignore\n!package.json\n"
    );
  }
  const distDir = join3(descriptorsDir, "dist");
  if (existsSync2(distDir)) {
    await fs2.rm(distDir, { recursive: true });
  }
}
async function addDescriptorsToPackageJson(path2) {
  const [packageJson, protocol] = await Promise.all([
    readPackage2(),
    getPackageProtocol()
  ]);
  const packageSource = `${protocol}:${path2}`;
  const currentSource = packageJson.dependencies?.["@polkadot-api/descriptors"];
  if (currentSource !== packageSource) {
    await updatePackage({
      dependencies: {
        "@polkadot-api/descriptors": packageSource
      }
    });
  }
}
async function getPackageProtocol() {
  const { packageManager, version } = await detectPackageManager();
  switch (packageManager) {
    case "yarn":
      const yarnMajorVersion = Number(version.split(".").at(0));
      return yarnMajorVersion >= 2 ? "portal" : "file";
    default:
      return "file";
  }
}
async function runInstall() {
  const { executable } = await detectPackageManager();
  console.log(`${executable} install`);
  const child = spawn(executable, ["install"], {
    stdio: "inherit",
    shell: true,
    env: {
      ...process2.env,
      PAPI_SKIP_GENERATE: "true"
    }
  });
  await new Promise((resolve) => child.on("close", resolve));
}
var generateMetadataExportFile = (input) => `const binMeta: string = "${Buffer.from(input).toString("base64")}"; export default binMeta;`;
async function outputCodegen(chains, outputFolder, clientPath, whitelist) {
  const {
    commonFileContent,
    descriptorsFileContent,
    descriptorTypesFiles,
    metadataTypes,
    typesFileContent,
    publicTypes
  } = generateMultipleDescriptors(
    chains,
    {
      client: clientPath,
      metadataTypes: "./metadataTypes",
      types: "./common-types",
      descriptorValues: "./descriptors",
      common: "./common"
    },
    {
      whitelist: whitelist ?? void 0
    }
  );
  const hash = h64(
    Binary.fromText(
      Array.from(metadataTypes.checksumToIdx.keys()).join("")
    ).asBytes()
  );
  const EntryPointsCodec = Vector(EntryPointCodec);
  const TypedefsCodec = Vector(TypedefCodec);
  const TypesCodec = Tuple(EntryPointsCodec, TypedefsCodec);
  await fs2.mkdir(outputFolder, { recursive: true });
  await fs2.writeFile(path.join(outputFolder, "common.ts"), commonFileContent);
  const metadataTypesBase64 = Buffer.from(
    TypesCodec.enc([metadataTypes.entryPoints, metadataTypes.typedefs])
  ).toString("base64");
  await fs2.writeFile(
    path.join(outputFolder, "metadataTypes.ts"),
    `
const content = "${metadataTypesBase64}"
export default content
    `
  );
  await fs2.writeFile(
    path.join(outputFolder, "descriptors.ts"),
    descriptorsFileContent
  );
  await fs2.writeFile(
    path.join(outputFolder, "common-types.ts"),
    typesFileContent
  );
  await Promise.all(
    chains.map((chain, i) => [
      fs2.writeFile(
        join3(outputFolder, `${chain.key}.ts`),
        descriptorTypesFiles[i].content
      ),
      fs2.writeFile(
        join3(outputFolder, `${chain.key}_metadata.ts`),
        generateMetadataExportFile(chain.metadataRaw)
      )
    ]).flat()
  );
  await generateIndex(
    outputFolder,
    chains.map((chain) => chain.key),
    publicTypes,
    Object.fromEntries(
      chains.filter((x) => x.codeHash).map((x) => [x.codeHash, x.key])
    )
  );
  return hash;
}
async function outputContractCodegen(contracts, outputFolder) {
  console.log("Generating smart contract types");
  const contractsFolder = join3(outputFolder, "contracts");
  if (!existsSync2(contractsFolder))
    await fs2.mkdir(contractsFolder, { recursive: true });
  const imports = [];
  for (const [key, metadata2] of Object.entries(contracts.ink ?? {})) {
    try {
      const types = generateInkTypes(
        getInkLookup(JSON.parse(await fs2.readFile(metadata2, "utf-8")))
      );
      await fs2.writeFile(join3(contractsFolder, `${key}.ts`), types);
      imports.push(`export { descriptor as ${key} } from './${key}'`);
    } catch (ex) {
      console.error("Exception when generating descriptors for contract " + key);
      console.error(ex);
    }
  }
  for (const [key, metadata2] of Object.entries(contracts.sol ?? {})) {
    try {
      const types = generateSolTypes(
        JSON.parse(await fs2.readFile(metadata2, "utf-8"))
      );
      await fs2.writeFile(join3(contractsFolder, `${key}.ts`), types);
      imports.push(`export { descriptor as ${key} } from './${key}'`);
    } catch (ex) {
      console.error("Exception when generating descriptors for contract " + key);
      console.error(ex);
    }
  }
  await fs2.writeFile(
    join3(contractsFolder, `index.ts`),
    imports.join("\n") + "\n"
  );
  fs2.appendFile(
    join3(outputFolder, "index.ts"),
    `
    export * as contracts from './contracts';
    `
  );
}
async function compileCodegen(packageDir) {
  const srcDir = join3(packageDir, "src");
  const outDir = join3(packageDir, "dist");
  if (await fsExists2(outDir)) {
    await fs2.rm(outDir, { recursive: true });
  }
  await tsup.build({
    target: "es2022",
    format: ["cjs", "esm"],
    entry: [path.join(srcDir, "index.ts").replaceAll(win32.sep, posix.sep)],
    loader: {
      ".scale": "binary"
    },
    platform: "neutral",
    outDir,
    outExtension: (ctx) => ({
      js: ctx.format === "esm" ? ".mjs" : ".js"
    })
  });
  tsc.build({
    basePath: srcDir,
    compilerOptions: {
      skipLibCheck: true,
      declaration: true,
      emitDeclarationOnly: true,
      target: "esnext",
      module: "esnext",
      moduleResolution: "node",
      resolveJsonModule: true,
      allowSyntheticDefaultImports: true,
      outDir
    }
  });
}
var cacheMetadataStr = `
export const getMetadata: (codeHash: string) => Promise<Uint8Array | null> = async (
  codeHash: string
)=> {
  try {
    return await metadatas[codeHash].getMetadata()
  } catch {}
  return null
}`;
var generateIndex = async (path2, keys, publicTypes, metadatas) => {
  const indexTs = [
    ...keys.flatMap((key) => [
      `import { default as ${key} } from "./${key}";`,
      `export { ${key} }`,
      `export type * from "./${key}";`
    ]),
    `export {`,
    publicTypes.join(", "),
    `} from './common-types';`,
    `const metadatas = {${Object.entries(metadatas).map(([codeHash, key]) => `["${codeHash}"]: ${key}`).join(",\n")}}`,
    cacheMetadataStr
  ].join("\n");
  await fs2.writeFile(join3(path2, "index.ts"), indexTs);
};
async function replacePackageJson(descriptorsDir, version) {
  await fs2.writeFile(
    join3(descriptorsDir, "package.json"),
    `{
  "version": "0.1.0-autogenerated.${version}",
  "name": "@polkadot-api/descriptors",
  "files": [
    "dist"
  ],
  "exports": {
    ".": {
      "types": "./dist/index.d.ts",
      "module": "./dist/index.mjs",
      "import": "./dist/index.mjs",
      "require": "./dist/index.js"
    },
    "./package.json": "./package.json"
  },
  "main": "./dist/index.js",
  "module": "./dist/index.mjs",
  "browser": "./dist/index.mjs",
  "types": "./dist/index.d.ts",
  "sideEffects": false,
  "peerDependencies": {
    "polkadot-api": ">=1.11.2"
  }
}
`
  );
}
async function readWhitelist(filename) {
  if (!await fsExists2(filename)) {
    throw new Error("Whitelist file not found: " + filename);
  }
  const tmpDir = await mkdtemp(join3(tmpdir(), "papi-"));
  try {
    await tsup.build({
      format: "esm",
      entry: {
        index: filename
      },
      outDir: tmpDir,
      outExtension() {
        return { js: ".mjs" };
      },
      silent: true
    });
    const { whitelist } = await import(join3(tmpDir, "index.mjs"));
    return whitelist;
  } finally {
    await rm(tmpDir, { recursive: true }).catch(console.error);
  }
}
async function flushBundlerCache() {
  try {
    const viteMetadata = join3(
      process2.cwd(),
      "node_modules",
      ".vite",
      "deps",
      "_metadata.json"
    );
    if (await fsExists2(viteMetadata)) {
      await rm(viteMetadata);
    }
  } catch (ex) {
    console.error(ex);
  }
}

// src/commands/add.ts
import { compactNumber } from "@polkadot-api/substrate-bindings";
import { fromHex } from "@polkadot-api/utils";
import { getMetadataFromRuntime } from "@polkadot-api/wasm-executor/node";
import * as fs3 from "fs/promises";
import ora from "ora";
import { join as join4 } from "path";
import { existsSync as existsSync3 } from "fs";
async function add(key, options) {
  const config = await readPapiConfig(options.config) ?? defaultConfig;
  const entries = config.entries;
  if (key in entries) {
    console.warn(`Replacing existing ${key} config`);
  }
  if (options.file) {
    entries[key] = {
      metadata: options.file
    };
  } else if (options.wasm) {
    const spinner = ora(`Loading metadata from runtime`).start();
    const metadataHex = (await fs3.readFile(options.wasm)).toString("hex");
    const opaqueMeta = fromHex(getMetadataFromRuntime(`0x${metadataHex}`));
    const metadataLen = compactNumber.dec(opaqueMeta);
    const compactLen = compactNumber.enc(metadataLen).length;
    if (opaqueMeta.length - compactLen !== metadataLen)
      throw new Error("Not able to retrieve runtime metadata");
    spinner.text = "Writing metadata";
    const metadataRaw = opaqueMeta.slice(compactLen);
    const filename = await storeMetadata(metadataRaw, key);
    spinner.succeed(`Metadata saved as ${filename}`);
    entries[key] = {
      metadata: filename
    };
  } else {
    const entry = entryFromOptions(options);
    entries[key] = entry;
    if (!options.noPersist) {
      const spinner = ora(`Loading metadata`).start();
      const { metadataRaw, genesis, codeHash } = await getMetadata(entry);
      spinner.text = "Writing metadata";
      const filename = await storeMetadata(metadataRaw, key);
      spinner.succeed(`Metadata saved as ${filename}`);
      entry.metadata = filename;
      entry.genesis = genesis;
      entry.codeHash = codeHash;
    }
  }
  await writePapiConfig(options.config, config);
  console.log(`Saved new spec "${key}"`);
  if (!options.skipCodegen) {
    generate({
      config: options.config
    });
  }
}
async function storeMetadata(metadata2, key) {
  const defaultFolder = join4(papiFolder, "metadata");
  if (!existsSync3(defaultFolder)) {
    await fs3.mkdir(defaultFolder, { recursive: true });
  }
  const filename = join4(defaultFolder, `${key}.scale`);
  await writeMetadataToDisk(metadata2, filename);
  return filename;
}
var entryFromOptions = (options) => {
  if (options.wsUrl) {
    return {
      wsUrl: options.wsUrl,
      at: options.at
    };
  }
  if (options.chainSpec) {
    return {
      chainSpec: options.chainSpec
    };
  }
  if (options.name) {
    return {
      chain: options.name
    };
  }
  throw new Error(
    "add command needs one source, specified by options -f -w -c or -n"
  );
};

// src/commands/ink.ts
import { existsSync as existsSync4 } from "fs";
import * as fs4 from "fs/promises";
import { join as join5 } from "path";
var ink = {
  async add(file, options) {
    const metadata2 = JSON.parse(await fs4.readFile(file, "utf-8"));
    delete metadata2.source?.wasm;
    const key = options.key || metadata2.contract.name;
    const config = await readPapiConfig(options.config) ?? defaultConfig;
    const inkConfig = config.ink ||= {};
    if (key in inkConfig) {
      console.warn(`Replacing existing ${key} config`);
    }
    const contractsFolder = join5(papiFolder, "contracts");
    if (!existsSync4(contractsFolder)) {
      await fs4.mkdir(contractsFolder, { recursive: true });
    }
    const fileName = join5(contractsFolder, key + ".json");
    await fs4.writeFile(fileName, JSON.stringify(metadata2, null, 2));
    inkConfig[key] = fileName;
    await writePapiConfig(options.config, config);
    if (!options.skipCodegen) {
      generate({
        config: options.config
      });
    }
  },
  async remove(key, options) {
    const config = await readPapiConfig(options.config) ?? defaultConfig;
    const inkConfig = config.ink ||= {};
    if (!(key in inkConfig)) {
      console.log(`${key} contract not found in config`);
      return;
    }
    const fileName = inkConfig[key];
    delete inkConfig[key];
    if (existsSync4(fileName)) {
      await fs4.rm(fileName);
    }
    await writePapiConfig(options.config, config);
    if (!options.skipCodegen) {
      generate({
        config: options.config
      });
    }
  }
};

// src/commands/remove.ts
async function remove(key, options) {
  const config = await readPapiConfig(options.config) ?? defaultConfig;
  const entries = config.entries;
  if (!(key in entries)) {
    throw new Error(`Key ${key} not set in polkadot-api config`);
  }
  delete entries[key];
  await writePapiConfig(options.config, config);
  console.log(`Removed chain "${key}" from config`);
  if (!options.skipCodegen) {
    generate({
      config: options.config
    });
  }
}

// src/commands/update.ts
import ora2 from "ora";
async function update(keysInput, options) {
  const config = await readPapiConfig(options.config) ?? defaultConfig;
  const { entries } = config;
  const keys = keysInput === void 0 ? Object.keys(entries) : keysInput.split(",");
  const updateByKey = async (key) => {
    if (!(key in entries)) {
      throw new Error(`Key ${key} not set in polkadot-api config`);
    }
    const { metadata: filename, ...entry } = entries[key];
    if (!filename) {
      if (keysInput !== void 0) {
        console.warn(`Key ${key} doesn't have a metadata file to update`);
      }
      return;
    }
    if ("at" in entry) {
      console.warn(
        `Key ${key} is based on a specific block ${entry.at}, skipping update`
      );
      return;
    }
    const metadata2 = await getMetadata(entry);
    if (!metadata2) {
      if (keysInput !== void 0) {
        console.warn(
          `Key ${key} doesn't have any external source to update from`
        );
      }
      return;
    }
    entries[key].genesis = metadata2.genesis;
    entries[key].codeHash = metadata2.codeHash;
    spinner.text = `Writing ${key} metadata`;
    await writeMetadataToDisk(metadata2.metadataRaw, filename);
    spinner.succeed(`${key} metadata updated`);
  };
  const spinner = ora2(`Updating`).start();
  await Promise.all(keys.map(updateByKey));
  await writePapiConfig(options.config, config);
  if (!options.skipCodegen) {
    console.log(`Updating descriptors`);
    await generate({
      config: options.config
    });
  }
  spinner.stop();
  console.log(`Updated chain(s) "${keys.join(", ")}"`);
}

// src/cli.ts
import { Option, program } from "@commander-js/extra-typings";
import * as knownChains2 from "@polkadot-api/known-chains";

// src/commands/sol.ts
import { existsSync as existsSync5 } from "fs";
import * as fs5 from "fs/promises";
import { join as join6 } from "path";
var sol = {
  async add(file, key, options) {
    const abi = JSON.parse(await fs5.readFile(file, "utf-8"));
    const config = await readPapiConfig(options.config) ?? defaultConfig;
    const solConfig = config.sol ||= {};
    if (key in solConfig) {
      console.warn(`Replacing existing ${key} config`);
    }
    const contractsFolder = join6(papiFolder, "contracts");
    if (!existsSync5(contractsFolder)) {
      await fs5.mkdir(contractsFolder, { recursive: true });
    }
    const fileName = join6(contractsFolder, key + ".json");
    await fs5.writeFile(fileName, JSON.stringify(abi, null, 2));
    solConfig[key] = fileName;
    await writePapiConfig(options.config, config);
    if (!options.skipCodegen) {
      generate({
        config: options.config
      });
    }
  },
  async remove(key, options) {
    const config = await readPapiConfig(options.config) ?? defaultConfig;
    const solConfig = config.sol ||= {};
    if (!(key in solConfig)) {
      console.log(`${key} contract not found in config`);
      return;
    }
    const fileName = solConfig[key];
    delete solConfig[key];
    if (existsSync5(fileName)) {
      await fs5.rm(fileName);
    }
    await writePapiConfig(options.config, config);
    if (!options.skipCodegen) {
      generate({
        config: options.config
      });
    }
  }
};

// src/cli.ts
function getCli({
  add: add2,
  generate: generate2,
  remove: remove2,
  update: update2,
  ink: ink2,
  version
}) {
  program.name("polkadot-api").description("Polkadot API CLI").version(version);
  const config = new Option("--config <filename>", "Source for the config file");
  const skipCodegen = new Option(
    "--skip-codegen",
    "Skip running codegen after adding"
  );
  const whitelist = new Option(
    "--whitelist <filename>",
    "Use whitelist file to reduce descriptor size"
  );
  program.command("generate", {
    isDefault: true
  }).description("Generate descriptor files").addOption(config).addOption(whitelist).action(generate2);
  program.command("add").description("Add a new chain spec to the list").argument("<key>", "Key identifier for the chain spec").addOption(config).option("-f, --file <filename>", "Source from metadata encoded file").option("-w, --wsUrl <URL>", "Source from websocket url").option("-c, --chainSpec <filename>", "Source from chain spec file").addOption(
    new Option("-n, --name <name>", "Source from a well-known chain").choices(
      Object.keys(knownChains2)
    )
  ).option("--wasm <filename>", "Source from runtime wasm file").option(
    "--at <block hash or number>",
    "Only for -w/--wsUrl. Fetch the metadata for a specific block or hash"
  ).option("--no-persist", "Do not persist the metadata as a file").addOption(skipCodegen).addOption(whitelist).action(add2);
  program.command("update").description("Update the metadata files and generate descriptor files").argument(
    "[keys]",
    "Keys of the metadata files to update, separated by commas. Leave empty for all"
  ).addOption(config).addOption(skipCodegen).addOption(whitelist).action(update2);
  program.command("remove").description("Remove a chain spec from the list").argument("<key>", "Key identifier for the chain spec").addOption(config).addOption(skipCodegen).addOption(whitelist).action(remove2);
  const inkCommand = program.command("ink").description("Add, update or remove ink contracts");
  inkCommand.command("add").description("Add or update an ink contract").argument("<file>", ".contract or .json metadata file for the contract").option("-k, --key <key>", "Key identifier for the contract").addOption(config).addOption(skipCodegen).addOption(whitelist).action(ink2.add);
  inkCommand.command("remove").description("Remove an ink contract").argument("<key>", "Key identifier for the contract to remove").addOption(config).addOption(skipCodegen).addOption(whitelist).action(ink2.remove);
  const solCommand = program.command("sol").description("Add, update or remove solidity contracts");
  solCommand.command("add").description("Add or update a solidity contract").argument("<file>", ".abi file for the contract").argument("<key>", "Key identifier for the contract").addOption(config).addOption(skipCodegen).addOption(whitelist).action(sol.add);
  solCommand.command("remove").description("Remove a solidity contract").argument("<key>", "Key identifier for the contract to remove").addOption(config).addOption(skipCodegen).addOption(whitelist).action(sol.remove);
  return program;
}

export {
  getMetadata,
  readPapiConfig,
  generate,
  add,
  ink,
  remove,
  update,
  getCli
};
//# sourceMappingURL=chunk-HQ3MI4Q4.js.map